In this project, I worked with two other Masters students to create a large, multi-platform system.

This project makes use of an Intel Realsense D435i depth camera attatched to a robot to make full 3D scans of a room.
The pointcloud scan data from this robot is then passed to a middleman server that applies various filters and algorithms to clean the data and then pass over to the VR environment.
This middleman server was also the part of the system that handled all robot controls and translated inputs from VR to useable commands.

The VR environment displays the 3D room scan and the robot to accurately immerse the user in the virtual room.
From here, the user can explore the scan and send commands to the robot. They have a menu on their hand that can make the robot move in any direction and take another scan, and they can also place flags for the robot to move to autonomously.

The applications for this project are extensive, for example controlling rovers on Mars or the Moon, or exploring damaged buildings after an earthquake to search for survivors.